{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87410f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To extract all the links on a website, write the following code in the console:\n",
    "\n",
    "var x = document.querySelectorAll(\"a\");\n",
    "var myarray = []\n",
    "for (var i=0; i<x.length; i++){\n",
    "var nametext = x[i].textContent;\n",
    "var cleantext = nametext.replace(/\\s+/g, ' ').trim();\n",
    "var cleanlink = x[i].href;\n",
    "myarray.push([cleantext,cleanlink]);\n",
    "};\n",
    "function make_table() {\n",
    "    var table = '<table><thead><th>Name</th><th>Links</th></thead><tbody>';\n",
    "   for (var i=0; i<myarray.length; i++) {\n",
    "            table += '<tr><td>'+ myarray[i][0] + '</td><td>'+myarray[i][1]+'</td></tr>';\n",
    "    };\n",
    " \n",
    "    var w = window.open(\"\");\n",
    "w.document.write(table); \n",
    "}\n",
    "make_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0dc8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcde324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a dataframe\n",
    "df = pd.read_csv('Links.csv')\n",
    "\n",
    "# Extract the 'URL' column into the target_urls variable\n",
    "target_urls = df['URL'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeefc830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nites\\AppData\\Local\\Temp\\ipykernel_13784\\3651952518.py:21: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)\n"
     ]
    }
   ],
   "source": [
    "# Set Chrome options to accept cookies\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--disable-infobars\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "chrome_options.add_argument(\"--profile-directory=Default\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--enable-automation\")\n",
    "chrome_options.add_argument(\"--enable-javascript\")\n",
    "chrome_options.add_argument(\"--accept-langauge=en\")\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode, i.e., without opening a browser window\n",
    "\n",
    "# Set the path to the ChromeDriver executable\n",
    "chrome_driver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Initialize the Chrome driver with the options\n",
    "driver = webdriver.Chrome(chrome_driver_path, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7c584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the screen resolution (you can adjust this as per your requirement)\n",
    "screen_width = 1920\n",
    "screen_height = 1080\n",
    "driver.set_window_size(screen_width, screen_height)\n",
    "\n",
    "# Create a directory to store the website images\n",
    "output_dir = 'images'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b65a91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file to store the classification results\n",
    "csv_file = 'classification_results.csv'\n",
    "def classify_content(content):\n",
    "    # Convert the content to lowercase for case-insensitive matching\n",
    "    content = content.lower()\n",
    "\n",
    "    # Define the keywords for different categories\n",
    "    e_commerce_keywords = ['Buy', 'Shop', 'Product', 'Purchase', 'Order', 'Add to Cart', 'Checkout', 'Discount', 'Sale', 'Deal', 'Shipping', 'Delivery', 'Online Store', 'Shopping', 'Ecommerce', 'Shop Now', 'Best Sellers', 'New Arrivals', 'Customer Reviews', 'Shopping Cart']\n",
    "    news_keywords = ['News', 'Headlines', 'Breaking news', 'Current affairs', 'World news', 'Local news', 'International news', 'Politics', 'Business', 'Technology', 'Science', 'Health', 'Entertainment', 'Sports', 'Opinion', 'Editorials', 'Features', 'Investigative reporting', 'Interviews', 'Analysis']\n",
    "    blog_keywords = ['Blog', 'Article', 'Post', 'Opinion', 'Thoughts', 'Personal', 'Experience', 'Journal', 'Tips', 'How-to', 'DIY', 'Lifestyle', 'Fashion', 'Travel', 'Food', 'Fitness', 'Health', 'Parenting', 'Beauty', 'Photography']\n",
    "    entertainment_keywords=['Entertainment', 'Movies', 'TV Shows', 'Music', 'Celebrities', 'Gossip', 'Film Reviews', 'Celebrity News', 'Concerts', 'Events', 'Humor', 'Comedy', 'Games', 'Online Streaming', 'Pop Culture', 'Entertainment News']\n",
    "    educational_keywords = ['Education', 'Online Learning', 'Courses', 'Tutorials', 'Study Materials', 'Research', 'Academic', 'E-Learning', 'Knowledge', 'Educational Resources', 'School', 'University', 'Student', 'Teacher', 'Learning Platform']\n",
    "    govt_websites = ['Government', 'act','Official', 'Public Services', 'Government Programs', 'Policies', 'Laws', 'Regulations', 'Public Records', 'Citizenship', 'Taxes', 'Healthcare', 'Education', 'Employment', 'Visa', 'Passport', 'Government Agencies']\n",
    "    \n",
    "    try:\n",
    "        # Check for keywords in the content to determine the category\n",
    "        if any(keyword in content for keyword in e_commerce_keywords):\n",
    "            return 'E-commerce'\n",
    "        elif any(keyword in content for keyword in news_keywords):\n",
    "            return 'News'\n",
    "        elif any(keyword in content for keyword in blog_keywords):\n",
    "            return 'Blog'\n",
    "        elif any(keyword in content for keyword in entertainment_keywords):\n",
    "            return 'Entertainment'\n",
    "        elif any(keyword in content for keyword in educational_keywords):\n",
    "            return 'Educational'\n",
    "        elif any(keyword in content for keyword in govt_keywords):\n",
    "            return 'Government'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "\n",
    "    except Exception as e:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b9c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot captured for https://www.bestbuy.com and saved as images/screenshot_0.png\n",
      "Screenshot captured for http://en.people.cn/ and saved as images/screenshot_1.png\n"
     ]
    }
   ],
   "source": [
    "# Loop through the target URLs and scrape content, capture screenshots, and perform classification\n",
    "results = []\n",
    "for i, url in enumerate(target_urls):\n",
    "    try:\n",
    "        # Open the URL\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Handle cookie consent pop-ups (if any)\n",
    "        try:\n",
    "            cookie_button = driver.find_element(By.XPATH, '//*[contains(text(), \"Accept Cookies\")]')\n",
    "            cookie_button.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Extract content from the webpage\n",
    "        content_elements = driver.find_elements(By.XPATH, '//p | //h1 | //h2 | //h3')  # Search for <p>, <h1>, <h2>, <h3> tags\n",
    "        content = None\n",
    "\n",
    "        # Iterate through the content elements\n",
    "        for element in content_elements:\n",
    "            element_text = element.text.strip()\n",
    "            if element_text:\n",
    "                content = ' '.join([element_text for element_text in element_text.split() if element_text.strip()])\n",
    "                break\n",
    "        \n",
    "        # Perform content classification\n",
    "        category = classify_content(content)\n",
    "        # Capture the screenshot of the entire page\n",
    "        screenshot_file = f'{output_dir}/screenshot_{i}.png'\n",
    "        driver.save_screenshot(screenshot_file)\n",
    "        print(f'Screenshot captured for {url} and saved as {screenshot_file}')\n",
    "#         if category != 'Unknown':\n",
    "#             # Capture the screenshot of the entire page\n",
    "#             screenshot_file = f'{output_dir}/screenshot_{i}.png'\n",
    "#             driver.save_screenshot(screenshot_file)\n",
    "#             print(f'Screenshot captured for {url} and saved as {screenshot_file}')\n",
    "#         else:\n",
    "#             print(f'Screenshot could not be captured for {url}')\n",
    "#             screenshot_file = None\n",
    "\n",
    "        results.append((url, category, content, screenshot_file))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {url}: {str(e)}')\n",
    "\n",
    "# Quit the driver and close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba148b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification results saved to classification_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Write the results to the CSV file\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Website', 'Category', 'Content', 'Screenshot File'])\n",
    "    for result in results:\n",
    "        # Encode the content to handle non-ASCII characters\n",
    "        encoded_content = result[2].encode('ascii', 'ignore').decode('ascii')\n",
    "        # Create a new tuple with the encoded content\n",
    "        encoded_result = (result[0], result[1], encoded_content, result[3])\n",
    "        writer.writerow(encoded_result)\n",
    "\n",
    "print(f'Classification results saved to {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9323c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Processing the screenshots taken\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Directory path of the website screenshots\n",
    "screenshot_dir = 'website_images'\n",
    "\n",
    "# Output directory path for preprocessed screenshots\n",
    "preprocessed_dir = 'preprocessed_images'\n",
    "os.makedirs(preprocessed_dir, exist_ok=True)\n",
    "\n",
    "# Define the desired size and resolution for the preprocessed screenshots\n",
    "desired_width = 1280\n",
    "desired_height = 720\n",
    "\n",
    "# Function to preprocess a single screenshot\n",
    "def preprocess_screenshot(filename):\n",
    "    if filename.endswith('.png'):\n",
    "        # Open the screenshot image\n",
    "        image_path = os.path.join(screenshot_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Resize the image while maintaining the aspect ratio\n",
    "        image.thumbnail((desired_width, desired_height))\n",
    "\n",
    "        # Determine the cropping area to remove irrelevant sections (e.g., browser tabs, sidebars)\n",
    "        # Adjust the cropping dimensions according to your specific requirements\n",
    "        crop_left = 0\n",
    "        crop_top = 100\n",
    "        crop_right = image.width\n",
    "        crop_bottom = image.height - 100\n",
    "\n",
    "        # Crop the image to focus on the main content area\n",
    "        image = image.crop((crop_left, crop_top, crop_right, crop_bottom))\n",
    "        \n",
    "        # Convert the image to RGB color mode if it's not already\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "\n",
    "        # Save the preprocessed image to the output directory\n",
    "        output_path = os.path.join(preprocessed_dir, filename)\n",
    "        image.save(output_path)\n",
    "        print(f'Preprocessed image saved: {output_path}')\n",
    "\n",
    "# Get the list of screenshot files in the directory\n",
    "screenshot_files = [filename for filename in os.listdir(screenshot_dir) if filename.endswith('.png')]\n",
    "\n",
    "# Process the screenshots using parallel processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    executor.map(preprocess_screenshot, screenshot_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824fc806",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
